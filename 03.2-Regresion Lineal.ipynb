{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión lineal\n",
    "---\n",
    "\n",
    "#### Resumen:\n",
    "\n",
    "##### Hipótesis:\n",
    "### $$ h_\\theta(x) = \\theta_0 + \\theta_1x $$\n",
    "\n",
    "##### Parámetros:\n",
    "### $$\\theta_0, \\theta_1$$\n",
    "\n",
    "##### Función de costo:\n",
    "### $$J(\\theta_0,\\theta_1) = {1\\over2m} \\sum_{i=1}^{n} (h_\\theta(x_i)-y_i)^2$$\n",
    "\n",
    "##### Objetivo:\n",
    "### $${min\\over{\\theta_0 \\theta_1}} J(\\theta_0,\\theta_1)$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descenso del gradiente\n",
    "\n",
    "A continuación aprenderemos un algoritmo llamado descenso del gradiente, el cuál utilizaremos para optimizar nuestra función de costo y hallar los parámetros correctos para nuestra hipótesis.\n",
    "\n",
    "Resulta que el algoritmo del descenso del gradiente es un algortimo más general y se utiliza no sólo en la regresión lineal. De hecho, se utiliza en y todo los algortimos de machine learning.\n",
    "\n",
    "Este es el planteamiento del problema:\n",
    "\n",
    "##### Tenemos alguna función: $J(\\theta_0,\\theta_1)$\n",
    "##### Queremos: ${min\\over\\theta_0\\theta_1} J(\\theta_0, \\theta_1)$\n",
    "\n",
    "* Iniciamos con valores aleatorios o cualquier valor específico para $\\theta_0$ y $\\theta_1$\n",
    "* Haremos pequeños cambios a $\\theta_0$ y $\\theta_1$ para reducir $J(\\theta_0,\\theta_1)$ hasta que obtengamos un mínimo global o tal vez un mínimo local de la función de costo.\n",
    "\n",
    "##### Algoritmo\n",
    "![](images/GD.png)\n",
    "\n",
    "$\\alpha$ es un número llamado <b>learning rate</b> o <b>índice de aprendizaje</b>, y lo que hace es básicamente, controlar que tan grande es un paso que damos con el descenso del gradiente. El término de la derivada valuada en uno de nuestros parámetros lo que hace es darnos el valor de la pendiente de la recta tangente a nuestra función de costo, esto básicamente lo que nos permite es saber si debemos aumentar o disminuir del valor del parámetros que estamos evaluando y en cuánto debemos modificarlo para encontrar el valor que estamos buscando que minimiza nuestra función."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No haremos enfásis en el cálculo de las derivadas de nuestra función de costo simplemente vamos a tomarlas y sustituirlas en nuestro algoritmo.\n",
    "\n",
    "![](images/GD2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente estamos listos para implementar nuestro modelo de regresión lineal y ajustar sus parámetros utilizando el algoritmo del descenso del gradiente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
